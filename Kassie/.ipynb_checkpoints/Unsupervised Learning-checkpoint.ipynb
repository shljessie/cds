{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kassiewang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kassiewang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                   #کرونا #خامنه‌ای_ویروس\n",
      "1        بیشتر ۵۰هزار ماسک، که توسط بچه‌های جهادی حاج ق...\n",
      "2        @Olympics have been postponed. Inevitable, sur...\n",
      "3        یه سری آدمای اطرافمون مثل ویروس کرونا هستن اگه...\n",
      "4        عامي روغتیا وزیر فیروزدین فیروز: ښايي ۱۶میلیون...\n",
      "                               ...                        \n",
      "15573    Impeachment was a dire distraction from corona...\n",
      "15574                                                  NaN\n",
      "15575    Let’s follow the guidance of the many reliable...\n",
      "15576    Google Will Begin Allowing Some Advertisers To...\n",
      "15577       California: “to guide coronavirus messaging.” \n",
      "Name: text, Length: 15578, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kassiewang/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/kassiewang/GitHub/cds/csv/megadataset.csv\")\n",
    "\n",
    "\n",
    "def content_extractor(content, start=None, end=None):\n",
    "    try:\n",
    "        if start and content and end:\n",
    "            builder = \"{}(.*)(?={})\".format(start, end)\n",
    "            pattern = re.compile(builder)\n",
    "            return pattern.search(content).group(0)\n",
    "        else:\n",
    "            return content\n",
    "    except Exception as e:\n",
    "        return content\n",
    "\n",
    "\n",
    "def tokenization_s(sentences):  # same can be achieved for words tokens\n",
    "    s_new = []\n",
    "    for sent in (sentences[:][0]):\n",
    "        s_token = sent_tokenize(sent)\n",
    "        if s_token != '':\n",
    "            s_new.append(s_token)\n",
    "    return s_new\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in (text[:][0]):  # this is Df_pd for Df_np (text[:])\n",
    "        new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text)  # remove punc.\n",
    "        new_text = re.sub(r'\\d+', '', new_text)  # remove numbers\n",
    "        new_text = new_text.lower()  # lower case, .upper() for upper\n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_data\n",
    "\n",
    "\n",
    "def tokenization_w(words):\n",
    "    w_new = []\n",
    "    for w in (words[:][0]):  # for NumPy = words[:]\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            w_new.append(w_token)\n",
    "    return w_new\n",
    "\n",
    "\n",
    "snowball = SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "def stemming(words):\n",
    "    new = []\n",
    "    stem_words = [snowball.stem(x) for x in (words[:][0])]\n",
    "    new.append(stem_words)\n",
    "    return new\n",
    "\n",
    "\n",
    "ser1 = df['text'].apply(\n",
    "    lambda x: content_extractor(x, \"start_text\", \"end_text\"))\n",
    "print(ser1)\n",
    "test_pd = pd.DataFrame(ser1)\n",
    "# text = pd.read_csv('twitter_text.csv', encoding='utf-8', header=None)\n",
    "# clean_test = preprocess(test_pd)\n",
    "# clean_words = tokenization_w(clean_test)\n",
    "# stem_test = stemming(clean_words)\n",
    "\n",
    "\n",
    "def text_process(text):\n",
    "    '''\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Return the cleaned text as a list of words\n",
    "    4. Remove words\n",
    "    '''\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
    "    nopunc = [word.lower() for word in nopunc.split()\n",
    "              if word not in stopwords.words('english')]\n",
    "    return [stemmer.lemmatize(word) for word in nopunc]\n",
    "\n",
    "\n",
    "#testing the function with a sample text#\n",
    "sample_text = \"Hey There! This is a Sample review, which 123happens {blah}%456 to contain happened punctuations universal rights of right contained.\"\n",
    "for index, row in islice(test_pd.iterrows(), 0, None):\n",
    "    new_entry = []\n",
    "    new_entry += [text_process(str(row['text']))]\n",
    "    processed_df = pd.DataFrame([new_entry], columns=['cleaned_text'])\n",
    "    df = df.append(processed_df, ignore_index=True)\n",
    "\n",
    "df.to_csv('preprocessed.csv', mode='w',\n",
    "          columns=['cleaned_text'], index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconvert = TfidfVectorizer(analyzer=text_process,ngram_range=(1,3)).fit(X_train)\n",
    "\n",
    "X_transformed=tfidfconvert.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "modelkmeans = KMeans(n_clusters=60, init='k-means++', n_init=100)\n",
    "modelkmeans.fit(X_transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
