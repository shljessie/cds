{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=60, n_init=100, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_process(text):\n",
    "    '''\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Return the cleaned text as a list of words\n",
    "    4. Remove words\n",
    "    '''\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
    "    nopunc = [word.lower() for word in nopunc.split()\n",
    "              if word not in stopwords.words('english')]\n",
    "    return [stemmer.lemmatize(word) for word in nopunc]\n",
    "\n",
    "train_df = pd.read_csv(\"cleaned.csv\")\n",
    "train_comments = train_df['cleaned_text']\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=text_process,ngram_range=(1,3))\n",
    "tfidfconvert = vectorizer.fit(train_comments)\n",
    "X_transformed = tfidfconvert.transform(train_comments)\n",
    "\n",
    "len(tfidfconvert.vocabulary_)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "modelkmeans = KMeans(n_clusters=60, init='k-means++', n_init=100)\n",
    "modelkmeans.fit(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered distribution of labels:\n",
      " 24    2188\n",
      "20    1209\n",
      "37     361\n",
      "5      331\n",
      "6      264\n",
      "54     258\n",
      "55     255\n",
      "43     248\n",
      "45     235\n",
      "4      231\n",
      "32     228\n",
      "30     217\n",
      "12     212\n",
      "21     205\n",
      "17     204\n",
      "42     199\n",
      "3      197\n",
      "52     196\n",
      "33     195\n",
      "50     190\n",
      "18     182\n",
      "10     181\n",
      "9      179\n",
      "15     178\n",
      "47     174\n",
      "49     173\n",
      "13     173\n",
      "25     172\n",
      "8      170\n",
      "38     167\n",
      "14     165\n",
      "36     154\n",
      "41     154\n",
      "2      153\n",
      "1      153\n",
      "29     148\n",
      "40     140\n",
      "0      138\n",
      "46     135\n",
      "16     134\n",
      "11     131\n",
      "22     129\n",
      "27     124\n",
      "59     120\n",
      "35     107\n",
      "31     104\n",
      "34     103\n",
      "7      101\n",
      "57      95\n",
      "48      88\n",
      "39      86\n",
      "26      84\n",
      "53      82\n",
      "58      79\n",
      "28      75\n",
      "56      53\n",
      "44      50\n",
      "19      49\n",
      "51      19\n",
      "23      17\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Clustered distribution of labels:\\n', pd.DataFrame(modelkmeans.labels_)[0].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
